{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca46afe3",
   "metadata": {},
   "source": [
    "#[문제]\n",
    "- law_2.docx 파일을 읽고, chroma 저장  \n",
    "- LLM질문 -> 답변  \n",
    "- 전세사기피해에 관한 법률 질문만 받기.\n",
    "- 이 외의 질문은 '답변을 할 수 없습니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fbaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"전세사기피해자\"는 전세사기로 인한 피해를 입은 임차인으로, 제3조의 요건을 모두 갖춘 후 전세사기피해지원위원회의 심의 및 의결을 거쳐 국토교통부장관이 결정한 임차인을 말합니다.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "\n",
    "## 1. 문서 내용 읽고 분할\n",
    "loader = Docx2txtLoader('law_2.docx') #인스턴스 생성 #문서 확인\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(  #분할을 하기위한 설정\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    ") #나중에 여러번의 질문을 통해 정확도를 확인\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter) #파일을 갖고 파라미터에 설정된 읽고 분할\n",
    "\n",
    "## 2. 임베딩 -> 벡터 데이터베이스에 저장 \n",
    "## 2.1. 환경변수 읽어오기\n",
    "load_dotenv() #잘 읽어왔다는 True\n",
    "\n",
    "## 2.2. 임베딩 모델 지정\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large') #성능 좋은 유료  # 위에서 OPENAI_API_KEY 읽었기 때문에 \n",
    "\n",
    "# # ## 2.3. 벡터 데이터베이스에 저장 ########한번 실행했기 때문에\n",
    "##[방법 1] in memory\n",
    "# database = Chroma.from_documents(\n",
    "#     documents=document_list,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory='./chroma',\n",
    "#     collection_name='chroma-law',\n",
    "# )\n",
    "\n",
    "## 로컬에 저장된 임베딩 데이터 가져오기\n",
    "\n",
    "database =Chroma(\n",
    "    collection_name='chroma-law',\n",
    "    persist_directory='./chroma',\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "\n",
    "\n",
    "## 3. 질문이 있으면, 벡터 데이터베이스(vector_store)에서 유사도 검색\n",
    "\n",
    "## 3.1. 사용자 질문\n",
    "# query = '오늘 서울 날씨는?'\n",
    "query = '전세사기 피해자는 누구인가요? '\n",
    "\n",
    "## 3.2. 벡터 데이터베이스에서 유사도 검색 (점수 포함)\n",
    "retrieved_docs = database.similarity_search(query=query, k=2)\n",
    "\n",
    "## 문서 객체에서 문자열만 추출\n",
    "\n",
    "\n",
    "\n",
    "## 3.3 문서 객체 -> 하나의 문자열\n",
    "## 리스트 for 문\n",
    "context = '🤷‍♀️🤷‍♀️\\n\\n'.join([doc.page_content for doc in retrieved_docs])\n",
    "context\n",
    "\n",
    "\n",
    "## 4.0 유사도 검색으로 가져온 문서를 LLM에 질문과 같이 전달 \n",
    "\n",
    "## 4.1. 프롬프트 작성\n",
    "prompt = '''\n",
    "[identity]\n",
    "- 당신은 전세사기 피해 법률 전문가입니다.\n",
    "- [context]를 참고하여 사용자의 질문에 답변해주세요.\n",
    "- 전세사기 피해 법률 이외의 질문에는 '답변을 할 수 없습니다.'로 답하세요.\n",
    "\n",
    "[context]\n",
    "{retrieved_docs}\n",
    "\n",
    "Question: {query}\n",
    "'''\n",
    "\n",
    "## 4.2. 프롬프트 변수에 값 설정\n",
    "formatted_prompt = prompt.format(\n",
    "    retrieved_docs=context,\n",
    "    query=query,\n",
    ")\n",
    "\n",
    "## 4.3. LLM 모델 생성(chatOpenAI 인스턴스 생성)  \n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "## 4.4. LLM 모델에 질문과 검색된 문서를 보냄  \n",
    "ai_message = llm.invoke(formatted_prompt)\n",
    "ai_message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
